{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "df = pd.read_csv(\"D:/r python/machine learning/data/Bike Share Demand/train.csv\",parse_dates=['datetime'])\n",
    "df['year'] = df['datetime'].dt.year\n",
    "df['month'] = df['datetime'].dt.month\n",
    "df['day'] = df['datetime'].dt.day\n",
    "df['hour'] = df['datetime'].dt.hour\n",
    "df['weekday']=df['datetime'].dt.weekday\n",
    "\n",
    "df['season'] = df['season'].astype('category')\n",
    "df['weather'] = df['weather'].astype('category')\n",
    "df.drop(columns=['datetime','count', 'registered'],inplace=True)\n",
    "\n",
    "dum_df = pd.get_dummies(df, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df_scaled=scaler.fit_transform(dum_df)\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "model = KMeans(n_clusters=4,random_state = 2019)\n",
    "\n",
    "model.fit(df_scaled)\n",
    "labels=pd.Series(model.predict(df_scaled)).rename('ClusterID')\n",
    "Clustered=pd.concat([dum_df,labels],axis='columns')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SS0 = Clustered[Clustered['ClusterID']==0]\n",
    "SS1 = Clustered[Clustered['ClusterID']==1]\n",
    "SS2 = Clustered[Clustered['ClusterID']==2]\n",
    "SS3 = Clustered[Clustered['ClusterID']==3]\n",
    "\n",
    "\n",
    "SSx0 = SS0.drop(['ClusterID','casual'],axis='columns')\n",
    "SSx1 = SS1.drop(['ClusterID','casual'],axis='columns')\n",
    "SSx2 = SS2.drop(['ClusterID','casual'],axis='columns')\n",
    "SSx3 = SS3.drop(['ClusterID','casual'],axis='columns')\n",
    "SSy0=SS0['casual']\n",
    "SSy1=SS1['casual']\n",
    "SSy2=SS2['casual']\n",
    "SSy3=SS3['casual']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model_rf = RandomForestRegressor(random_state=2019,\n",
    "                                  n_estimators=500,oob_score=True)\n",
    "m1=model_rf.fit(SSx0,SSy0)\n",
    "m2=model_rf.fit(SSx1,SSy1)\n",
    "m3=model_rf.fit(SSx2,SSy2)\n",
    "m4=model_rf.fit(SSx3,SSy3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"D:/r python/machine learning/data/Bike Share Demand/test.csv\",parse_dates=['datetime'])\n",
    "df2['year'] = df2['datetime'].dt.year\n",
    "df2['month'] = df2['datetime'].dt.month\n",
    "df2['day'] = df2['datetime'].dt.day\n",
    "df2['hour'] = df2['datetime'].dt.hour\n",
    "df2['weekday']=df2['datetime'].dt.weekday\n",
    "\n",
    "df2['season'] = df2['season'].astype('category')\n",
    "df2['weather'] = df2['weather'].astype('category')\n",
    "datetimeOriginal = df2['datetime']\n",
    "df2.drop(columns=['datetime'],inplace=True)\n",
    "\n",
    "dum_df2 = pd.get_dummies(df2, drop_first=True)\n",
    "\n",
    "X2 = dum_df2\n",
    "X2scaled=scaler.fit_transform(X2)\n",
    "\n",
    "model = KMeans(n_clusters=4,random_state = 2019)\n",
    "model.fit(X2scaled)\n",
    "labels2 = pd.Series( model.predict(X2scaled)).rename('ClusterID')\n",
    "\n",
    "X2Clustered = pd.concat([X2,labels2,datetimeOriginal],axis='columns')\n",
    "SS20 = X2Clustered[X2Clustered['ClusterID']==0]\n",
    "SS21 = X2Clustered[X2Clustered['ClusterID']==1]\n",
    "SS22 = X2Clustered[X2Clustered['ClusterID']==2]\n",
    "SS23 = X2Clustered[X2Clustered['ClusterID']==3]\n",
    "\n",
    "### For preserving the datetime sequences\n",
    "SS20_datetimeOriginal = SS20['datetime']\n",
    "SS21_datetimeOriginal = SS21['datetime']\n",
    "SS22_datetimeOriginal = SS22['datetime']\n",
    "SS23_datetimeOriginal = SS23['datetime']\n",
    "\n",
    "SS20 = SS20.drop(['ClusterID','datetime'],axis='columns')\n",
    "SS21 = SS21.drop(['ClusterID','datetime'],axis='columns')\n",
    "SS22 = SS22.drop(['ClusterID','datetime'],axis='columns')\n",
    "SS23 = SS23.drop(['ClusterID','datetime'],axis='columns')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predm1 = model_rf.predict(SS20)\n",
    "y_predm1[y_predm1<0] = 0\n",
    "y_predm2 = model_rf.predict(SS21)\n",
    "y_predm2[y_predm2<0] = 0\n",
    "y_predm3 = model_rf.predict(SS22)\n",
    "y_predm3[y_predm3<0] = 0\n",
    "y_predm4 = model_rf.predict(SS23)\n",
    "y_predm4[y_predm4<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust0=pd.DataFrame({'datetime':SS20_datetimeOriginal,'casual':y_predm1})\n",
    "clust1=pd.DataFrame({'datetime':SS21_datetimeOriginal,'casual':y_predm2})\n",
    "clust2=pd.DataFrame({'datetime':SS22_datetimeOriginal,'casual':y_predm3})\n",
    "clust3=pd.DataFrame({'datetime':SS23_datetimeOriginal,'casual':y_predm4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit=pd.concat([clust0,clust1,clust2,clust3],ignore_index=True)\n",
    "submit2=submit.sort_values(by=['datetime'])\n",
    "\n",
    "submit2.to_csv(\"D:/r python/machine learning/data/Bike Share Demand/fi.csv\",\n",
    "              index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
